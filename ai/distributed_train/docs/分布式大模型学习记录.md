# 分布式大模型训练学习记录



## 资料整理

### 文档

#### [LLM分布式训练并行技术](https://github.com/liguodongiot/llm-action/tree/main/docs/llm-base/distribution-parallelism)

- [大模型分布式训练并行技术（一）-概述](https://zhuanlan.zhihu.com/p/598714869)
- [大模型分布式训练并行技术（二）-数据并行](https://zhuanlan.zhihu.com/p/650002268)
- [大模型分布式训练并行技术（三）-流水线并行](https://zhuanlan.zhihu.com/p/653860567)
- [大模型分布式训练并行技术（四）-张量并行](https://zhuanlan.zhihu.com/p/657921100)
- [大模型分布式训练并行技术（五）-序列并行](https://zhuanlan.zhihu.com/p/659792351)
- [大模型分布式训练并行技术（六）-多维混合并行](https://zhuanlan.zhihu.com/p/661279318)
- [大模型分布式训练并行技术（七）-自动并行](https://zhuanlan.zhihu.com/p/662517647)
- [大模型分布式训练并行技术（八）-MOE并行](https://zhuanlan.zhihu.com/p/662518387)
- [大模型分布式训练并行技术（九）-总结](https://juejin.cn/post/7290740395913969705)



### 分布式AI框架

- [PyTorch](https://github.com/liguodongiot/llm-action/tree/main/train/pytorch/)
  - PyTorch 单机多卡训练
  - PyTorch 多机多卡训练
- [Megatron-LM](https://github.com/liguodongiot/llm-action/tree/main/train/megatron)
  - Megatron-LM 单机多卡训练
  - Megatron-LM 多机多卡训练
  - [基于Megatron-LM从0到1完成GPT2模型预训练、模型评估及推理](https://juejin.cn/post/7259682893648724029)
- [DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/deepspeed)
  - DeepSpeed 单机多卡训练
  - DeepSpeed 多机多卡训练
- [Megatron-DeepSpeed](https://github.com/liguodongiot/llm-action/tree/main/train/megatron-deepspeed)
  - 基于 Megatron-DeepSpeed 从 0 到1 完成 LLaMA 预训练
  - 基于 Megatron-DeepSpeed 从 0 到1 完成 Bloom 预训练




## 学习记录



### 机器学习基础概念



-   什么是前向传播

-   什么是反向传播

-   什么是梯度

-   什么是平均梯度

-   什么是模型权重

    

### 概述

分布式训练有哪些并行技术：

-   数据并行
-   模型模型
    -   流水线并行
    -   张量并行
-   序列并行
-   多维混合并行
-   自动并行
-   MOE并行



### 数据并行

Q：什么是数据并行。

答：数据并行，数据集被分割成几个碎片，每个碎片被分配到一个设备上。这相当于沿批次（Batch）维度对训练过程进行并行化。每个设备将持有一个完整的模型副本，并在分配的数据集碎片上进行训练。在反向传播之后，模型的梯度将被全部减少，以便在不同设备上的模型参数能够保持同步。



Q:数据并行有什么用

A: 



Q: 怎么实现数据并行

A: 典型的数据并行实现：[PyTorch DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)



Q:数据并行有什么优缺点

A: 优点：简单。缺点：每个 GPU 持有整个模型权重的副本导致冗余问题。





### 模型并行

Q： 什么是模型并行，模型并行解决了什么问题，有何优缺点，如何实现。

答：模型被分割并分布在一个设备阵列上。



Q：模型并行解决了什么问题

A：解决了冗余问题。



Q: 模型并行的优缺点

A: 



Q:模型并行的实现

A：有两种实现，分别是张量并行，流水线并行。



Q：什么是张量并行

A：张量并行训练是将一个张量沿特定维度分成 N 块，每个设备只持有整个张量的 1/N，同时不影响计算图的正确性。



Q:张量并行解决了什么问题

A：



Q：张量并行的优缺点

A：



Q:张量并行的实现

A：典型的张量并行实现：[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)（1D）、[Colossal-AI](https://github.com/hpcaitech/ColossalAI)（2D、2.5D、3D）。





Q: 什么是流水线并行

A：流水线并行的核心思想是，模型按层分割成若干块，每块都交给一个设备。在前向传播过程中，每个设备将中间的激活传递给下一个阶段。在后向传播过程中，每个设备将输入张量的梯度传回给前一个流水线阶段。



Q：流水线并行的优缺点

A：优点：允许设备同时进行计算，从而增加训练的吞吐量。 缺点：训练设备容易出现空闲状态，导致计算资源的浪费，加速效率没有数据并行高。



Q：如何实现流水线并行
A：典型的流水线并行实现：[GPipe](https://github.com/kakaobrain/torchgpipe)、[PipeDream](https://github.com/msr-fiddle/pipedream)、PipeDream-2BW、PipeDream Flush（1F1B）。



### 优化器相关的并行

Q:什么是优化器相关的并行

A：随着模型越来越大，单个GPU的显存目前通常无法装下那么大的模型了。那么就要想办法对占显存的地方进行优化。通常来说，模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。如下图

![img](https://danerlt-1258802437.cos.ap-chongqing.myqcloud.com/images/v2-0767b38b6144986667975d2b99d02bc3.webp)

可以看到模型参数仅占模型训练过程中所有数据的一部分，当进行混合精度运算时，其中模型状态参数(优化器状态 + 梯度+ 模型参数）占到了一大半以上。因此，我们需要想办法去除模型训练过程中的冗余数据。

而优化器相关的并行就是一种去除冗余数据的并行方案，目前这种并行最流行的方法是 ZeRO（即零冗余优化器）。针对模型状态的存储优化（去除冗余），ZeRO使用的方法是分片，即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态。ZeRO有三个不同级别，对模型状态进行不同程度的分片：

- ZeRO-1 : 对优化器状态分片（Optimizer States Sharding）
- ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding）
- ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding）



Q：优化器并行的优缺点

A：



Q: 优化器并行如何实现

A：



### 异构系统并行

Q：什么是异构系统并行

A：利用CPU和硬盘来训练



Q:异构系统并行解决了什么问题

A：使用异构系统有可能在一台服务器上容纳一个巨大的模型。



Q:异构系统并行的优缺点

A：



Q：如何实现异构系统并行

A：



### 多维混合并行













## 代码



[llm-action项目](https://github.com/liguodongiot/llm-action.git)



### 环境搭建

创建 `conda` 环境

```bash
conda create -n dis python=3.10
conda activate dis
```

`pytorch `和 `cuda` 版本对应关系参考链接： [https://docs.nvidia.com/deploy/cuda-compatibility/](https://docs.nvidia.com/deploy/cuda-compatibility/) ，这里使用`cuda 11.7`,  `pytorch 2.0.1`

下载`pytorch` whl包: [下载地址](https://download.pytorch.org/whl/cu117)

安装`pytorch`：

```bash
pip install torch-2.0.1+cu117-cp310-cp310-linux_x86_64.whl -i https://mirrors.aliyun.com/pypi/simple/
pip install torchaudio-2.0.2+cu117-cp310-cp310-linux_x86_64.whl -i https://mirrors.aliyun.com/pypi/simple/
pip install torchvision-0.15.2+cu117-cp310-cp310-linux_x86_64.whl -i https://mirrors.aliyun.com/pypi/simple/
```

验证 `torch` 是否安装成功，`torch.cuda.is_available()`返回 `True` 表示安装成功。:

```python
$ python
Python 3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> print(torch.__version__)
2.0.1+cu117
>>> torch.cuda.is_available()
True
>>> 
```

如果出现下面错误，说明环境变量有问题

```
ImportError: /miniconda3/envs/dis/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: symbol cublasSetWorkspace_v2, version libcublas.so.11 not defined in file libcublas.so.11 with link time reference
```

需要将 `~/.bahsrc`文件中的环境变量设置成：

```bash
export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
```

安装`deepspeed`

```bash
pip install deepspeed -i https://mirrors.aliyun.com/pypi/simple/
```

验证拾安装成功：

```
ds_report
[2023-12-06 16:24:02,687] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [OKAY]
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-devel package with yum
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [NO] ....... [NO]
fused_adam ............. [NO] ....... [OKAY]
cpu_adam ............... [NO] ....... [OKAY]
cpu_adagrad ............ [NO] ....... [OKAY]
cpu_lion ............... [NO] ....... [OKAY]
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [NO] ....... [NO]
fused_lamb ............. [NO] ....... [OKAY]
fused_lion ............. [NO] ....... [OKAY]
inference_core_ops ..... [NO] ....... [OKAY]
cutlass_ops ............ [NO] ....... [OKAY]
quantizer .............. [NO] ....... [OKAY]
ragged_device_ops ...... [NO] ....... [OKAY]
ragged_ops ............. [NO] ....... [OKAY]
random_ltd ............. [NO] ....... [OKAY]
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
 [WARNING]  using untested triton version (2.0.0), only 1.0.0 is known to be compatible
sparse_attn ............ [NO] ....... [NO]
spatial_inference ...... [NO] ....... [OKAY]
transformer ............ [NO] ....... [OKAY]
stochastic_transformer . [NO] ....... [OKAY]
transformer_inference .. [NO] ....... [OKAY]
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/data/user/miniconda3/envs/dis/lib/python3.10/site-packages/torch']
torch version .................... 2.0.1+cu117
deepspeed install path ........... ['/data/user/miniconda3/envs/dis/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.12.4, unknown, unknown
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.4
deepspeed wheel compiled w. ...... torch 2.0, cuda 11.7
shared memory (/dev/shm) size .... 125.70 GB

```









## 验证代码







## 参考链接

-   [PyTorch Distributed overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
-   [PyTorch Getting started with distributed data parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
-   [使用 PyTorch 编写分布式应用程序](https://pytorch.org/tutorials/intermediate/dist_tuto.html) 
-   [llm-action项目](https://github.com/liguodongiot/llm-action.git) by [liguodongiot](https://github.com/liguodongiot)
-   [deepspeed安装踩坑实践](https://zhuanlan.zhihu.com/p/617491455) by [Firefly](https://www.zhihu.com/people/fireflycsq)
-   [Pytorch - 分布式通信原语（附源码）](https://zhuanlan.zhihu.com/p/532129899) by [极市平台](https://www.zhihu.com/org/ji-shi-jiao-14)
-   [分布式训练 – 第1篇 - 什么是分布式训练](https://www.changping.me/2021/04/18/ai-distributed-training-whatistraining/) by [常平](https://www.changping.me/about/)
-   [分布式训练 – 第3篇 - 集合通信及其通信原语](https://www.changping.me/2022/04/04/ai-distributed-training-coll-lang/) by [常平](https://www.changping.me/about/)
-   [分布式训练 - 第5篇 - 分布式训练服务框架基本原理与架构解析](https://www.changping.me/2022/04/17/ai-distributed-training-framework-1/) by [常平](https://www.changping.me/about/)
-   [horovod: Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.](https://github.com/horovod/horovod) by [horovod](https://github.com/horovod)
-   [Pytorch DDP分布式训练介绍](https://shomy.top/2022/01/05/torch-ddp-intro/) by [天空的城](https://shomy.top/about//)
-   [分布式训练硬核技术——通信原语](https://zhuanlan.zhihu.com/p/465967735) by [ZOMI醤](https://www.zhihu.com/people/ZOMII)
-   [DeepSpeed介绍](https://zhuanlan.zhihu.com/p/624412809) by [菩提树](https://www.zhihu.com/people/lu-zhu-yi-yi-64)
-   [深度学习超大模型的分布式训练的探索（一）](https://zhuanlan.zhihu.com/p/376538786) by [秃顶的码农](https://www.zhihu.com/people/dbkcpp)
-   [分布式训练](https://openmlsys.github.io/chapter_distributed_training/index.html) by [《机器学习系统：设计和实现书籍》](https://openmlsys.github.io/)
-   [常见的分布式并行策略](https://docs.oneflow.org/master/parallelism/01_introduction.html) by [OneFlow](https://github.com/OneFlow-Inc/oneflow)
-   [[源码解析] PyTorch 分布式(5) ------ DistributedDataParallel 总述&如何使用](https://www.cnblogs.com/rossiXYZ/p/15553384.html)

